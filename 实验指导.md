# Lab6: RNN综合实验指导

> 配套代码：`Rnn Experiment Framework`（PyTorch）与 `Rnn Scratch Framework`（NumPy）

------

[TOC]

------

## 1. 实验目标与知识点

| 目标                   | 关键点                                      |
| ---------------------- | ------------------------------------------- |
| 掌握 RNN/LSTM 理论     | 递归公式、BPTT、梯度爆炸/消失现象及缓解策略 |
| 熟悉字符级语言模型     | one-hot/embedding、交叉熵损失、温度采样     |
| 独立手写 RNN           | 前向 + BPTT + 参数更新全过程                |
| 提升调参与实验报告能力 | 曲线可视化、案例分析、批评性思考            |

------

## 2. 原理回顾

### 2.1 Vanilla RNN 数学形式

$$h_t = \tanh( W_{xh} x_t + W_{hh} h_{t-1} + b_h )$$

$$\hat y_t = \mathrm{softmax}( W_{hy} h_t + b_y )$$

- **循环依赖**：$h_t$ 同时依赖当前输入 $x_t$ 与前一时刻隐藏状态 $h_{t-1}$。
- **参数共享**：所有时间步使用同一组权重，实现序列长度不定的建模。

### 2.2 反向传播 Through Time（BPTT）

- **展开计算图**：将序列视为深度 = $T$ 的前馈网络。
- **梯度公式**（链式法则）：

$$
\frac{\partial\mathcal L}{\partial W_{hh}} = \sum_{t=1}^T \delta_t\, h_{t-1}^T, \qquad
\delta_t = \bigl( W_{hh}^T\,\delta_{t+1} + W_{hy}^T (\hat y_t - y_t) \bigr) \odot (1-h_t^2)
$$

其中 $\odot$ 为 Hadamard 乘。

- **梯度爆炸/消失**：当 $|W_{hh}|*2 \gg 1$ 时梯度指数级爆炸；当 $|W*{hh}|_2 \ll 1$ 时迅速衰减。
- **缓解方法**：
  1. 梯度裁剪（本实验中 `--clip`）。
  2. 门控单元（LSTM/GRU）。

### 2.3 LSTM 门控机制

- LSTM 通过 **输入门** $i_t$、**遗忘门** $f_t$、**输出门** $o_t$ 与 **候选记忆** $\tilde c_t$ 控制信息流：

  $$i_t = \sigma\!\bigl(W_{xi}x_t + W_{hi}h_{t-1}+b_i\bigr),
  \quad f_t = \sigma(\cdot),
  \quad o_t = \sigma(\cdot)$$

  $$\tilde c_t = \tanh\!\bigl(W_{xc}x_t + W_{hc}h_{t-1}+b_c\bigr)$$

  $$c_t = f_t \odot c_{t-1} + i_t \odot \tilde c_t$$

  $$h_t = o_t \odot \tanh(c_t)$$

- **关键信号通路**：细胞状态 $c_t$ 仅受按元素乘控制，避免反复线性变换，从而 **稳定梯度**。

### 2.4 采样温度与多样性

对 softmax logits 除以温度 $T$：$P(i) \propto \exp(z_i/T)$。

- $T<1$ → 分布更尖锐，输出更确定，易重复。
- $T>1$ → 分布更平坦，文本更有创造性但也可能语法混乱。

### 2.5 实践技巧

| 问题         | 技巧                                    |
| ------------ | --------------------------------------- |
| 长序列训练慢 | 截断 BPTT：只保留最近 $k$ 步梯度        |
| 首字难预测   | 使用前缀 *seed* 引导。                  |
| 模型过拟合   | Dropout、增大语料、多层较浅替代单层巨大 |

------

## 3. 环境与数据准备


| 项目                   | 版本/要求                            |
| ---------------------- | ------------------------------------ |
| Python                 | ≥ 3.8                                |
| 必要库（PyTorch 路径） | `numpy matplotlib torch requests`    |
| 可选库（实验报告）     | `pandas seaborn jupyter`             |
| IDE                    | VS Code / PyCharm / Jupyter Notebook |

> **纯 NumPy 路径** 仅依赖 `numpy matplotlib requests`，无需安装 PyTorch。

**GPU** 非必须，但若可用推荐使用以加速 PyTorch 实验。

------

## 4. 实验内容


### 4.1 A — PyTorch 高阶版

1. 运行 baseline。
2. **原理提醒**：关注 `Embedding` 的作用 —— 将 one-hot 稀疏向量映射到稠密语义空间；这相当于学习字符级“词向量”。
3. 观察损失曲线：与理论中交叉熵 $\mathcal L=-\sum y\log\hat y$ 对应。
4. 改写 `train()` 引入 `torch.nn.utils.clip_grad_norm_` 的 **阈值对梯度爆炸的抑制**。
5. 对比不同温度采样结果，回到 §2.4 解释差异。

### 4.2 B — NumPy 手搓版

1. 补全 `RNN.forward`：按公式实现隐藏状态递推。
    *思考*: 为什么最后仅用 $h_T$ 而非整条序列？（提示：字符级语言模型希望预测下一个字符，可改为全时间步预测提高训练信号密度）
2. 补全 `backward`：沿时间反向循环调用 `cell.backward`。
3. 实现自己的 **梯度检查**：对比数值梯度与解析梯度，验证 BPTT 正确。
4. **原理提醒**：手动实现梯度让你更直观感受爆炸/消失，可尝试打印 `\|\delta_t\|_2` 熵随 $t$ 的衰减。

------

## 5. 实验报告 & 评分

| 项目       | 权重 | 评判要点                                 |
| ---------- | ---- | ---------------------------------------- |
| 代码正确性 | 30   | PyTorch 与 NumPy 均能正常训练 & 生成文本 |
| 报告完整度 | 25   | 模块齐全、逻辑清晰、图片/表格排版良好    |
| 结果分析   | 25   | 合理解释现象，并用数据/示例支撑          |
| 创新扩展   | 10   | 早停、LR 调度、GRU/Bi‑RNN、Adam NumPy 等 |
| 规范与整洁 | 10   | 代码注释、变量命名、报告格式             |


## 6. 常见问题排查

| 现象               | 原理解释                     | 解决                               |
| ------------------ | ---------------------------- | ---------------------------------- |
| 损失 NaN / 爆炸     | 学习率过大 / 梯度爆炸 | 减小 `--lr`；调高 `--clip`             |
| 训练过慢            | CPU 运行 / 批量过小   | 使用 GPU；增大 `--batch`；裁剪序列长度 |
| 生成文本重复/无意义 | 训练不足 / 模型容量低 | 增加 epoch；扩大隐藏层；调高温度       |
| 采样时输出全同字符 | $T\to0$ 导致几乎贪心，熵过低 | 调高温度 0.8–1.2                   |
| 损失振荡不收敛     | 梯度爆炸 + 学习率过高        | 使用梯度裁剪、学习率热重启或 AdamW |

------

## 7. 拓展阅读与思考题

1. **Pascanu et al. (2013)** – 如何系统地缓解梯度爆炸？阅读后简述 Clip-by-Norm 的理论推导。
2. 门控结构对信息流的控制与 **残差连接** 有何异同？
3. 将字符级模型升级到 **子词 BPE** 是否能改善生成质量？思考其对嵌入层、输出层大小的影响。

------

## 8. 提交方式与截止日期

代码打包成zip文件，实验报告以PDF格式上交

截止日期为实验发布后两周

> **祝学习愉快，Keep Coding & Thinking!**

